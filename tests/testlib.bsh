#!/usr/bin/env false bash

#*# tests/testlib

#**
# ================
# VSI Test Library
# ================
#
# .. default-domain:: bash
#
# .. file:: testlib.bsh
#
# Simple shell command language test library
#
# :Copyright: Original version: (c) 2011-13 by Ryan Tomayko <http://tomayko.com>
#
#             License: MIT
# :Author: Ryan Tomayko
#
# ## Writing unit tests
#
# Test Lib gives you are number of basic unit test functionality from bash, including:
#
# - Running tests in subshells to prevent environment variable pollution.
# - An automatically self deleting :envvar:`TRASHDIR` for all the tests
# - An automatically self deleting :envvar:`TESTDIR` for each individual test
# - A tally of successful run and failed tests, and additionally expected failures, unexpected successes, required failures, and skipped tests
# - Individual est times :envvar:`TESTLIB_SHOW_TIMING`
# - :func:`setup` - A user defined function run before the first test
# - :func:`teardown` - A user defined function after the last test
# - Keep temporary directories for debugging :envvar:`TESTLIB_KEEP_TEMP_DIRS`
# - Pause before deleting temporary directories if there is is a failure, for inspection :envvar:`TESTLIB_KEEP_PAUSE_AFTER_ERROR`
# - Run only a single test by it's description :envvar:`TESTLIB_RUN_SINGLE_TEST`
# - Regular expression to skip tests by description :envvar:`TESTLIB_SKIP_TESTS`
# - Control stderr/stdout redirection :envvar:`TESTLIB_REDIRECT_OUTPUT`
# - Stop testing after ``N`` failures :envvar:`TESTLIB_STOP_AFTER_FAILS`
# - Disable custom PS4 in trace :envvar:`TESTLIB_NO_PS4`
#
# .. rubric:: Example
#
# .. code-block:: bash
#   :caption: Tests must follow the basic form:
#
#   source testlib.bsh
#
#   begin_test "the thing"
#   (
#        set -e
#        echo "hello"
#        [ 1 == 1 ] # this is ok
#        # However, the following needs "|| false" because on bash 3.2 and 4.0
#        # there is a bug where [[ ]] will fail, and bash knows it fails ($?),
#        # but "set -e" does not count this as an error. This is fixed in bash 4.1
#        [[ 1 == 1 ]] || false
#        false
#   )
#   end_test
#
#   Any command that evaluates to false "fails" the test. When a test fails its stdout, stderr, and call trace are printed out.
#
# .. rubric:: Bugs
#
# On darling: when debugging a unit test error, sometimes the printout is cut off, making it difficult to do "printf debugging." While the cause and scope of this is unknown, a work around that sometimes works is
#
# .. code-block:: bash
#
#     runtests 2>&1 | less -R
#
# ## Test status
#
# Every test will print out a line with its name, and the status of the test run.
#
# Possible results of a test:
#
# - ``OK`` - The test passed!
# - ``FAILED`` - The test did not pass. Trace is printed out for debugging.
# - ``SETUP FAILURE`` - Each test must call :func:`setup_test`, and it was not detected for this test.
# - ``SKIPPED`` - The test was not run.
# - ``FAIL REQUIRED`` - A test successfully failed as it is required to.
# - ``FAIL EXPECTED`` - A test successfully failed as it is expected to.
# - ``SHOULD HAVE FAILED ELSEWHERE`` - A required or expected failed test failed in an the wrong area of the code. There is something wrong with the test, and a trace is printed out for debugging.
# - ``SHOULD HAVE FAILED`` - A required failed test did not fail. There is something wrong with the test, and a trace is printed out for debugging.
# - ``UNEXPECTED SUCCESS`` - An expected failed test never actually failed. This doesn't count as a failure, but a middle ground in its own category.
# - ``REQUIRED FAILURE SETUP ERROR`` - A required failed test did not call :func:`begin_fail_zone`, and is considered setup incorrectly.
# - ``EXPECTED FAILURE SETUP ERROR`` - An expected failed test did not call :func:`begin_fail_zone`, and is considered setup incorrectly.
#**

if [ -z ${VSI_COMMON_DIR+set} ]; then
  VSI_COMMON_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.."; pwd)"
fi

set -e

if [ "${TESTLIB_SHOW_TIMING-0}" == "1" ] || [[ ${OSTYPE} = darwin* ]]; then
  . "${VSI_COMMON_DIR}/linux/time_tools.bsh"
fi
. "${VSI_COMMON_DIR}/linux/file_tools.bsh"
. "${VSI_COMMON_DIR}/linux/dir_tools.bsh"
. "${VSI_COMMON_DIR}/tests/test_colors.sh"
. "${VSI_COMMON_DIR}/linux/signal_tools.bsh"

# create a temporary work space
TRASHDIR="$(mktemp -d -t $(basename "$0")-$$.XXXXXXXX)"

# keep track of num tests and failures
tests=0
failures=0
expected_failures=0
unexpected_successes=0
required_failures=0
skipped=0

#**
# .. function:: setup
#
# Function run before the first test
#
# .. note::
#   A directory :envvar:`TRASHDIR` is created for setup, right before running :func:`setup` ().
#
#   Setup is not run if no tests are ever run
#**

#**
# .. envvar:: TRASHDIR
#
# Temporary directory where everything for the test file is stored
#
# Automatically generated and removed (unless :envvar:`TESTLIB_KEEP_TEMP_DIRS` is changed)
#
# .. seealso::
#   :envvar:`TESTDIR`
#**

#**
# .. envvar:: TESTDIR
#
# Unique temporary directory for a single test (in :envvar:`TRASHDIR`)
#
# Automatically generated and removed (unless :envvar:`TESTLIB_KEEP_TEMP_DIRS` is changed)
#
# .. seealso::
#   :envvar:`TRASHDIR`
#**

#**
# .. function:: teardown
#
# Function run after the last test
#
# .. note::
#   Teardown is not run if no tests are ever run
#**

#**
# .. envvar:: TESTLIB_KEEP_TEMP_DIRS
#
# Keep the trashdir/setup dir
#
# Debug flag to keep the temporary directories generated when testing. Set to ``1`` to keep directories.
#
# :Default: ``0``
#**

#**
# .. envvar:: TESTLIB_KEEP_PAUSE_AFTER_ERROR
#
# Pauses before allowing testlib to cleanup and delete all temporary files.
#
# A better alternative to :envvar:`TESTLIB_KEEP_TEMP_DIRS`, so that you are given time to investigate a problem, and can then press any key to cleanup the temporary directory
#
# :Default: ``0``
#**

#**
# .. envvar:: TESTLIB_SHOW_TIMING
#
# Display test time after each test
#
# Debug flag to display time elapsed for each test. Set to ``1`` to enable.
#
# :Default: ``0``
#**

#**
# .. envvar:: TESTLIB_RUN_SINGLE_TEST
#
# Run a single test
#
# Instead of running all the tests in a test file, only the tests with a description exactly matching the value of :envvar:`TESTLIB_RUN_SINGLE_TEST` will be run. Useful for debugging a specific test/piece of code
#
# :Default: *unset*
#**

#**
# .. envvar:: TESTLIB_SKIP_TESTS
#
# A bash regex expressions that designates tests to be skipped.
#
# :Default: *unset*
#
# .. rubric:: Examples
#
# .. code:: bash
#
#    TESTLIB_SKIP_TESTS='^New Just$|foo'
#    # Skip "New Just" and anything with "foo" it is, e.g. "food"
#**

#**
# .. envvar:: TESTLIB_REDIRECT_OUTPUT
#
# Redirects stdout and stderr to temporary files
#
# By default, all tests are run with ``set -xv`` for debugging purposes when a tests fails. This output is stored in a out/err/xtrace file temporarily and only displayed if a tests fails. You can set this variable to control the streams to always output.
#
# :Values: * ``3`` Redirect stdout, stderr, and xtrace
#          * ``2`` Redirect stderr, and xtrace, but let stdout through
#          * ``1`` Redirect xtrace, but let stdout and stderr through. On bash 4.0 and older, this will let xtrace through too
#          * ``0`` Let everything through
#
# :Default: ``3``
#**

: ${TESTLIB_REDIRECT_OUTPUT=3}

#**
# .. envvar:: TESTLIB_NO_PS4
#
# If set, will disable the custom PS4 output. Useful for some coverage tools
#
# :Default: *unset*
#**

#**
# .. envvar:: TESTLIB_STOP_AFTER_FAILS
#
# If set, stops after this many tests have fails in a single file. Instead of running the rest of the tests in a file, they are skipped.
#
# :Default: ``0`` - Unlimited
#**

#**
# .. function:: atexit
#
# Function that runs at process exit
#
# .. rubric:: Usage
#
# Automatically called on exit by trap.
#
# Checks to see if :func:`teardown` is defined, and calls it. :func:`teardown` is typically a function, alias, or something that makes sense to call.
#**
atexit ()
{
  test_status=$?
  if [ -n "${test_description+set}" ]; then
    end_test $test_status
    echo "${TESTLIB_BAD_COLOR}WARNING${TESTLIB_RESET_COLOR}: end_test not added at end of last test." 1>&2
  fi
  unset test_status

  if [ "${tests}" -ne 0 ] && [ "${tracking_touched_files-}" = "1" ]; then
    cleanup_touched_files
  fi

  if [ "${tests}" -ne 0 ] && type -t teardown &> /dev/null && [ "$(command -v teardown)" == "teardown" ]; then
    teardown
  fi


  if [ "${TESTLIB_KEEP_TEMP_DIRS-}" != "1" ]; then
    # Only works if stdin is connected, aka only one test file on run_tests
    if [ "${failures}" != "0" -a "${TESTLIB_KEEP_PAUSE_AFTER_ERROR-0}" = "1" ]; then
      read -rsn1 -p "Press any key to continue..." x
    fi
    rm -rf "$TRASHDIR"
  fi

  local BOLD_COLOR
  if [ "${failures}" -eq 0 ]; then
    BOLD_COLOR="${TESTLIB_BOLD_COLOR}"
  else
    BOLD_COLOR="${TESTLIB_BAD_COLOR}"
  fi

  printf "%s summary: %d tests, ${BOLD_COLOR}%d failures${TESTLIB_RESET_COLOR}, %d unexpected successes, %d expected failures, %d required failures, %d skipped\n" \
         "$0" "${tests}" "${failures}" "${unexpected_successes}" "${expected_failures}" "${required_failures}" "${skipped}"

  if [ -d "${TESTLIB_SUMMARY_DIR-}" ]; then
    echo "${tests} ${failures} ${unexpected_successes} ${expected_failures} ${required_failures} ${skipped}" > "${TESTLIB_SUMMARY_DIR}/$(basename "$0")"
  fi

  if [ "${failures}" -gt 0 ]; then
    exit 1
  else
    exit 0
  fi
}
trap_chain "atexit" EXIT


if [ -z "${TESTLIB_NO_PS4+set}" ]; then
  if declare -p BASH_SOURCE &> /dev/null; then
    PS4=$'+${BASH_SOURCE[0]##*/}:${LINENO})\t'
  else # Else sh probably
    # Not as accurate, but better than nothing
    PS4=$'+${0##*/}:${LINENO})\t'
  fi
fi

# Common code for begin tests
_begin_common_test ()
{
  local fd
  TESTDIR="${TRASHDIR}/test${tests}"
  mkdir -p "${TESTDIR}"
  pushd "$TESTDIR" &> /dev/null
  # This makes calling end_test between tests "optional", but highly recommended.
  # end_test does have to be called after the last test, especially if teardown
  # is defined after the last test
  if [ -n "${test_description+set}" ]; then
    end_test $test_status
    echo "${TESTLIB_BAD_COLOR}WARNING${TESTLIB_RESET_COLOR}: end_test not added at end of a test." 1>&2
    declare -p BASH_SOURCE
  fi
  unset test_status

  # Set flag defaults that could be overridable in certain test types.
  # This needs to be after end_test call above in order to keep end_test
  # optional
  expected_failure=${_expected_failure-0}
  required_fail=${_required_fail-0}

  # Run setup if this is the first test
  if [ "${tests}" -eq 0 ] && type -t setup &> /dev/null && [ "$(command -v setup)" == "setup" ]; then
    setup
  fi

  tests=$(( tests + 1 ))
  local test_file_name="$(basename "$0")"
  test_file_name=${test_file_name%.*}
  test_file_name=${test_file_name#test-}
  test_description="$test_file_name - $1"

  if [ "${tracking_touched_files-}" = "1" ]; then
    track_touched_file="$(mktemp -u)"
    ttouch "${track_touched_file}"
  fi

  if [ "${TESTLIB_SHOW_TIMING-0}" = "1" ]; then
    _time_0=$(get_time_seconds)
  fi

  if [ "${TESTLIB_RUN_SINGLE_TEST+set}" = "set" ] && \
     [ "$1" != "${TESTLIB_RUN_SINGLE_TEST}" ]; then
    skip_next_test
  fi

  if [ "${BASH_VERSINFO[0]}${BASH_VERSINFO[1]}" -ge "41" ]; then
    case ${TESTLIB_REDIRECT_OUTPUT} in
      3)
        exec {stdout}>&1 {stderr}>&2
        ;;
      2)
        exec {stderr}>&2
        ;;
    esac
    xtrace="${TESTDIR}/xtrace"
    if [ "${TESTLIB_REDIRECT_OUTPUT}" -ge "1" ]; then
      exec {BASH_XTRACEFD}>"${xtrace}"
    fi
  else
    if [ "${TESTLIB_REDIRECT_OUTPUT}" -ge "3" ]; then
      find_open_fd stdout
      eval "exec ${stdout}>&1"
    fi
    if [ "${TESTLIB_REDIRECT_OUTPUT}" -ge "2" ]; then
      find_open_fd stderr
      eval "exec ${stderr}>&2"
    fi
    # Doesn't support BASH_XTRACEFD
    xtrace="${TESTDIR}/err"
  fi

  out="${TESTDIR}/out"
  err="${TESTDIR}/err"

  case "${TESTLIB_REDIRECT_OUTPUT}" in
    3)
      exec 1>"$out" 2>"$err"
      ;;
    2)
      exec 2>"$err"
      ;;
  esac

  if [ -n "${TESTLIB_SKIP_TESTS+set}" ] && [[ ${1} =~ ${TESTLIB_SKIP_TESTS} ]]; then
    __testlib_skip_test=1
  fi

  # Allow the subshell to exit non-zero without exiting this process
  set -x +e
}

#**
# .. function:: begin_test
#
# Beginning of test demarcation
#
# .. rubric:: Usage
#
# Mark the beginning of a test. A subshell should immediately follow this statement.
#
# .. seealso::
#   :func:`end_test`
#**
begin_test ()
{
  test_status=$? # Must be first command
  _begin_common_test ${@+"${@}"}
}

#**
# .. function:: begin_expected_fail_test
#
# Beginning of expected fail test demarcation
#
# .. rubric:: Usage
#
# Define the beginning of a test that is expected to fail. Failures may only occur in "fail zones" denoted by :func:`begin_fail_zone` and :func:`end_fail_zone`
#**
begin_expected_fail_test()
{
  test_status=$? # Must be first command
  # Override _begin_common_test default
  _expected_failure=1 _begin_common_test ${@+"${@}"}
}

#**
# .. function:: begin_fail_zone
#
# Start a fail zone
#
# In practice, having an expected fail or required fail leads to the possibility of a test failing somewhere you don't expect it to. For this reason, fail zones must be denoted in order for :func:`begin_expected_fail_test` and :func:`begin_required_fail_test` test to to succeed, and the the failures must occur in them only. If a test a failure happens outside the fail zone, the test will be marked as a failure, with the message ``SHOULD HAVE FAILED ELSEWHERE``.
#
# .. rubric:: Example
#
# .. code-block:: bash
#
#   begin_expected_fail_test "Some test"
#   (
#     setup_test
#     # Failing here would result in failure
#     begin_fail_zone
#     false is ok here
#   )
#**
begin_fail_zone()
{
  echo 1 > "${TRASHDIR}/.testlib_failure_zone"
}

#**
# .. function:: end_fail_zone
#
# End a fail zone
#
# While not common, it might be possible to have a test that is likely to fail in one of many places, for this reason a fail zone can be turned off, before being turned on again.
#
# .. rubric:: Example
#
# .. code-block:: bash
#
#   begin_required_fail_test "Some test"
#   (
#     setup_test
#     true something
#
#     begin_fail_zone
#     maybe_false
#     end_fail_zone
#
#     true again
#
#     begin_fail_zone
#     false_if_other_was_not
#     # end_fail_zone # not required at the end of the test, but won't hurt
#   )
#   end_test
#**
end_fail_zone()
{
  echo 0 > "${TRASHDIR}/.testlib_failure_zone"
}

#**
# .. function:: begin_required_fail_test
#
# Beginning of required fail test demarcation
#
# .. rubric:: Usage
#
# Define the beginning of a test that is required to fail
#**
begin_required_fail_test()
{
  test_status=$? # Must be first command
  # Override _begin_common_test default
  _required_fail=1 _begin_common_test ${@+"${@}"}
}

#**
# .. function:: setup_test
#
# Sets up the test
#
# Once inside the () subshell, typically set -eu needs to be run, then other things such as checking to see if a test should be skipped, etc. need to be done. This is all encapsulated into :func:`setup_test`. This is required; without it, :func:`end_test` will know you forgot to call this and fail.
#
# This is also the second part of creating a skippable test.
#
# You are free to change "set -eu" after :func:`setup_test`, should you wish.
#
# .. rubric:: Usage
#
# Place at the beginning of a test
#
# .. rubric:: Example
#
# .. code-block:: bash
#
#   skip_next_test
#   begin_test "Skipping test"
#   (
#     setup_test
#     #test code here
#   )
#
# .. seealso::
#   :func:`skip_next_test`
#**
setup_test()
{
  # Identify that setup_test was called
  touch "${TRASHDIR}/.setup_test"

  # Check to see if this test should be skipped
  if [ "${__testlib_skip_test-}" = "1" ]; then
    exit 0
  fi

  set -eu
}

#**
# .. function:: end_test
#
# End of a test demarcation
#
# .. rubric:: Usage
#
# Mark the end of a test. Must be the first command after the test group, or else the return value will not be captured successfully.
#
# .. seealso::
#   :func:`begin_test`
#**
end_test ()
{
  test_status="${1:-$?}" # This MUST be the first line of this function
  set +x -e
  case "${TESTLIB_REDIRECT_OUTPUT}" in
    3) # Tested, this apparently works on bash 3.2
      exec 1>&${stdout} 2>&${stderr}
      ;;
    2)
      exec 2>&${stderr}
      ;;
  esac
  if [ "${TESTLIB_REDIRECT_OUTPUT}" -ge "1" -a "${xtrace}" != "${err}" ]; then
    # if these are different files, close xtrace, should only be here in bash
    # 4.1 or newer, so no need to waste time checking.
    exec {BASH_XTRACEFD}>&-
  fi
  popd &> /dev/null

  local time_e=''
  if [ "${TESTLIB_SHOW_TIMING-0}" = "1" ]; then
    time_e=$(awk "BEGIN {print \"\t\" $(get_time_seconds)-${_time_0}}")
  fi

  # Handle missing call to setup_test
  if [ ! -e "${TRASHDIR}/.setup_test" ]; then
    # This is a "no matter what, failure". No expected/required failure work
    # around for this
    printf "%-80s ${TESTLIB_BAD_COLOR}SETUP FAILURE${TESTLIB_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
    failures=$(( failures + 1 ))
  # Handle a skipped test
  elif [ "${__testlib_skip_test-}" = "1" ] && [ "${test_status}" -eq 0 ]; then
    printf "%-80s ${TESTLIB_GOOD_COLOR}SKIPPED${TESTLIB_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
    skipped=$((skipped+1))
  # Handle a required fail
  elif [ "${required_fail}" -eq 1 ]; then
    if [ -f "${TRASHDIR}/.testlib_failure_zone" ]; then
      local required_failure_state="$(<"${TRASHDIR}/.testlib_failure_zone")"
      if [ "$test_status" != 0 ]; then
        if [ "${required_failure_state}" = "1" ]; then
          printf "%-80s ${TESTLIB_GOOD_COLOR}FAIL REQUIRED${TESTLIB_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
          required_failures=$((required_failures+1))
        else
          printf "%-80s ${TESTLIB_BAD_COLOR}SHOULD HAVE FAILED ELSEWHERE${TESTLIB_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
          failures=$(( failures + 1 ))
          testlib_print_test_trace
        fi
      else
        failures=$(( failures + 1 ))
        printf "%-80s ${TESTLIB_BAD_COLOR}SHOULD HAVE FAILED${TESTLIB_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
        testlib_print_test_trace
      fi
    else
      # Handle missing call to begin_fail_zone
      printf "%-80s ${TESTLIB_BAD_COLOR}REQUIRED FAILURE SETUP ERROR${TESTLIB_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
      failures=$(( failures + 1 ))
    fi
  # Handle an expected fail
  elif [ "${expected_failure}" -eq 1 ]; then
    if [ -f "${TRASHDIR}/.testlib_failure_zone" ]; then
      local expected_failure_state="$(<"${TRASHDIR}/.testlib_failure_zone")"
      if [ "${test_status}" != "0" ]; then
        if [ "${expected_failure_state}" = "1" ]; then
          printf "%-80s ${TESTLIB_GOOD_COLOR}FAIL EXPECTED${TESTLIB_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
          expected_failures=$(( expected_failures + 1 ))
        else
          printf "%-80s ${TESTLIB_BAD_COLOR}SHOULD HAVE FAILED ELSEWHERE${TESTLIB_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
          failures=$(( failures + 1 ))
          testlib_print_test_trace
        fi
      else
        printf "%-80s ${TESTLIB_WARN_COLOR}UNEXPECTED SUCCESS${TESTLIB_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
        unexpected_successes=$(( unexpected_successes + 1 ))
      fi
    else
      # Handle missing call to begin_fail_zone
      printf "%-80s ${TESTLIB_BAD_COLOR}EXPECTED FAILURE SETUP ERROR${TESTLIB_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
      failures=$(( failures + 1 ))
    fi
    # Remove account file
    rm -f "${TRASHDIR}/.testlib_failure_zone"
  elif [ "${required_fail}" -eq 0 ] && [ "$test_status" -eq 0 ]; then
    printf "%-80s ${TESTLIB_GOOD_COLOR}OK${TESTLIB_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
  else
    printf "%-80s ${TESTLIB_BAD_COLOR}FAILED${TESTLIB_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
    failures=$(( failures + 1 ))
    testlib_print_test_trace
  fi

  if [ "${tracking_touched_files-}" = "1" ]; then
    cleanup_touched_files
  fi

  unset test_description
  unset __testlib_skip_test

  rm "${TRASHDIR}/.setup_test" || :

  if [ "${TESTLIB_STOP_AFTER_FAILS-0}" != "0" ] && [ "${failures}" -ge "${TESTLIB_STOP_AFTER_FAILS}" ]; then
    # Skip the rest of the tests.
    TESTLIB_SKIP_TESTS='.*'
  fi
}

function testlib_print_test_trace()
{
  local test_output
  # Darling has issue printing out too fast https://github.com/darlinghq/darling/issues/640
  # This perl command can slow it down enough that it works on my computer
  if command -v sw_vers &> /dev/null && [ "$(sw_vers -buildVersion)" = "Darling" ]; then
    find_open_fd test_output
    # Technically, the process substitution happens before the eval, but that
    # doesn't matter in the end, still works correctly.
    eval "exec ${test_output}> >(perl -e 'print && select undef,undef,undef,0.0001 while <>;' >&2 )"
  elif [ "${BASH_VERSINFO[0]}${BASH_VERSINFO[1]}" -ge "41" ]; then
    exec {test_output}>&2
  else
    find_open_fd test_output
    eval "exec ${test_output}>&2"
  fi

  (
    if [ "${TESTLIB_REDIRECT_OUTPUT}" -ge "1" ]; then

      if [ "${err}" = "${xtrace}" ]; then
        echo "-- stderr --"
      else
        echo "-- xtrace --"
      fi
      grep -v -e $'^\+[^\t]*\tend_test' \
              -e $'^\+[^\t]*\tset +x -e' <"${xtrace}" |
        sed $'s|^[^+]| \t&|' |
        column -c1 -s $'\t' -t |
        sed 's/^/    /'
    fi
    if [ "${TESTLIB_REDIRECT_OUTPUT}" -ge "2" ]; then
      if [ "${err}" != "${xtrace}" ]; then
        echo "-- stderr --"
        sed 's/^/    /' <"${err}"
      fi
    fi
    if [ "${TESTLIB_REDIRECT_OUTPUT}" -ge "3" ]; then
      echo "-- stdout --"
      sed 's/^/    /' <"${out}"
    fi
    echo "-- EOF $test_description --"
  ) 1>&${test_output}

  # Close fd test_output, which will end perl if its running
  if [ "${BASH_VERSINFO[0]}${BASH_VERSINFO[1]}" -ge "41" ]; then
    exec {test_output}>&-
  else
    eval "exec ${test_output}>&-"
  fi
}

#**
# .. function:: skip_next_test
#
# Function to indicate the next test should be skipped
#
# This is the first part of creating a skippable test, used in conjunction with :func:`setup_test`
#
# .. rubric:: Example
#
# .. code-block:: bash
#
#   For example, skip if docker command not found
#
#     command -v docker &> /dev/null && skip_next_test
#     begin_test "My test"
#     (
#       setup_test
#       [ "$(docker run -it --rm ubuntu:14.04 echo hi)" = "hi" ]
#     )
#
# .. seealso::
#   :func:`setup_test`
#
# .. note::
#   This must be done outside of the test, or else the skip variable will not be set and detected by :func:`end_test`
#**
skip_next_test()
{
  __testlib_skip_test=1
}

#**
# .. function:: not
#
# :Arguments: ``$1``... - Command and arguments
# :Output: Return value
#
#     * ``0`` - On non-zero return code evaluation
#     * ``1`` - On zero return code
#
# Returns true only when the command fails
#
# Since ``!`` is ignored by "set -e", use :func:`not` instead. This is just a helper to make unittests look nice and not need extra ifs everywhere
#
# .. rubric:: Example
#
# .. code-block:: bash
#
#   # No good, always passes, even if ! true
#   ! false
#
#   # good
#   not false
#   # equivalent to
#   if ! false; then
#     true
#   else
#     false
#   fi
#
# .. rubric:: Bugs
#
# Complex statements do not work, e.g. [, [[ and ((, etc...
#   For example, you should use
#
# .. code-block:: bash
#
#     [ ! -e /test ]
#
# |  instead of
#
# .. code-block:: bash
#
#     not [ -e /test ]
#
# |  In cases where this is not easily worked around, you can use
#
# .. code-block:: bash
#
#     not_s '[ -e /test ]'
#
# .. seealso::
#   :func:`not_s`
#**
not()
{
  local cmd="$1"
  shift 1
  if "${cmd}" ${@+"${@}"}; then
    return 1
  else
    return 0
  fi
}

# Testing this idea...
#**
# .. function:: not_s
#
# :Arguments: ``$1`` - Command/statement in a single string
# :Output: Return Value:
#
#             * ``0`` - On non-zero return code evaluation
#             * ``1`` - On zero return code
#
# Returns true only when the string version of command fails
#
# Since ``!`` is ignored by "set -e", use :func:`not` instead. This is just a helper to make unittests look nice and not need extra ifs everywhere.
#
# .. rubric:: Example
#
# .. code-block:: bash
#
#   x=test
#   y=t.st
#   not2 '[[ $x =~ $y ]]' # <-- notice single quotes.
#
# While the single quotes aren't necessary, they handle the more complicated situations more easily.
#
# .. note::
#   Uses eval
#
# .. seealso::
#   :func:`not`
#
#**
not_s()
{
  eval "if ${1}; then return 1; else return 0; fi"
}

#**
# .. function:: track_touched_files
#
# Start tracking touched files
#
# After running :func:`track_touched_files`, any call to touch will cause that file to be added to the internal list (touched_files). Just prior to the teardown phase, all of these files will be automatically removed for your convenience.
#
# .. rubric:: Example
#
# .. code-block:: bash
#
#   setup()
#   {
#     track_touched_files
#   }
#   begin_test Testing
#   (
#     touch /tmp/hiya
#   )
#   end_test
#
# .. rubric:: Usage
#
# Should be called before the :func:`begin_test` block, not inside. Inside a () subshell block will not work. Setup is the logical place to put it.
#
# .. rubric:: Bugs
#
# Does not work in sh, only ``bash``. Uses array, and I didn't want to make this use a string instead.
#
# Not thread safe. Use a different file for each thread
#
# .. seealso::
#   :func:`cleanup_touched_files`
#**
track_touched_files()
{
  tracking_touched_files=1
}

#**
# .. function:: ttouch
#
# Touch function that should behave like the original touch command
#
# .. seealso::
#   :func:`track_touched_files`
#**
ttouch()
{
  local filename
  local end_options=0

  touch "${@}"

  # Skip all options
  while [ $# -gt 0 ]; do
    if [ "${end_options}" = "0" ] && [ "$1" = "--" ]; then
      end_options=1
      shift 1
    elif [ "${end_options}" = "0" ] && [ ${#1} -gt 0 ] && [ "${1:0:1}" = "-" ]; then
      shift 1
    else
      filename="$1"
      # force to be absolute path
      if [ "${filename:0:1}" != "/" ]; then
        filename="$(pwd)/$1"
      fi
      printf "${filename}\0" >> "${track_touched_file}"
      shift 1
    fi
  done
}

#**
# .. function:: cleanup_touched_files
#
# Delete all the touched files
#
# At the end of the last test, delete all the files in the array
#**
cleanup_touched_files()
{
  local touched_file
  local touched_files
  local line

  # This will normally get called an extra time at exit, so the existence of
  # the file acts as a check.
  if [ -f "${track_touched_file}" ]; then
    while IFS='' read -r -d '' touched_file 2>/dev/null || [ -n "$touched_file" ]; do
      if [ -e "${touched_file}" ] && [ ! -d "${touched_file}" ]; then
        \rm "${touched_file}"
      fi

      touched_file='' #Have to clear it, in case the timeout times out
    done < "${track_touched_file}"
  fi
}